behaviors:
  MyAgent:
    trainer_type: ppo
    max_steps: 5000000
    time_horizon: 64        # Increased from 32 - better for navigation tasks
    hyperparameters:
      batch_size: 512       # Increased from 256 - more stable gradients
      buffer_size: 4096     # Increased from 2048 - more diverse experiences
      learning_rate: 1.0e-4 # Reduced from 3.0e-4 - more stable learning
      beta: 1.0e-4          # Reduced from 5.0e-4 - less random exploration
      epsilon: 0.15         # Reduced from 0.2 - smaller policy updates
      lambd: 0.95           # Reduced from 0.99 - faster credit assignment
      num_epoch: 3
      learning_rate_schedule: linear
    network_settings:
      hidden_units: 128     # Increased from 64 - more capacity for ray perception
      num_layers: 2
      normalize: true
    reward_signals:
      extrinsic:
        gamma: 0.99
        strength: 1.0
    summary_freq: 5000
    checkpoint_interval: 25000


    # behavioral_cloning:
    #   demo_path: G:\unity_files\thesis-1\Assets\Demonstrations\drone.demo
    #   strength: 0.5  
    #   steps: 50000  
      
# Removed behavioral_cloning to use 32x32 resolution instead of 84x84
# mlagents-learn config\trainer_config.yaml --run-id=MyFirstRun --train


# [INFO] Exported results\drone2.4\MyAgent\MyAgent-2899974.onnx

